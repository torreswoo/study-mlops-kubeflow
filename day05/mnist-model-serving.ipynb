{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sample for KFServing SDK\n",
    "\n",
    "KFServing SDK 샘플입니다. \n",
    "\n",
    "interenceService를 만들고 가져오고 학습한 모델을 배포하고 삭제하는 예제가 있는 노트북입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "\n",
    "from kfserving import KFServingClient\n",
    "from kfserving import constants\n",
    "from kfserving import utils\n",
    "from kfserving import V1alpha2EndpointSpec\n",
    "from kfserving import V1alpha2PredictorSpec\n",
    "from kfserving import V1alpha2TensorflowSpec\n",
    "from kfserving import V1alpha2InferenceServiceSpec\n",
    "from kfserving import V1alpha2InferenceService\n",
    "from kubernetes.client import V1ResourceRequirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "infereceService를 배포할 네임스페이스를 정합니다. \n",
    "아레는 default namespace를 지정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = utils.get_default_target_namespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Define InferenceService\n",
    "\n",
    "먼저 default endpoint spec을 정의합니다. 그리고 inferenceservice의 기본적인 spec을 정의합니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = constants.KFSERVING_GROUP + '/' + constants.KFSERVING_VERSION\n",
    "default_endpoint_spec = V1alpha2EndpointSpec(\n",
    "                          predictor=V1alpha2PredictorSpec(\n",
    "                            tensorflow=V1alpha2TensorflowSpec(\n",
    "                              storage_uri='pvc://workspace-lecture-tf/saved_model',\n",
    "                              resources=V1ResourceRequirements(\n",
    "                                  requests={'cpu':'100m','memory':'1Gi'},\n",
    "                                  limits={'cpu':'100m', 'memory':'1Gi'}))))\n",
    "    \n",
    "isvc = V1alpha2InferenceService(api_version=api_version,\n",
    "                          kind=constants.KFSERVING_KIND,\n",
    "                          metadata=client.V1ObjectMeta(\n",
    "                              name='mnist-server', namespace=namespace),\n",
    "                          spec=V1alpha2InferenceServiceSpec(default=default_endpoint_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create InferenceService\n",
    "\n",
    "inferenceSerivce를 만들기 위해 KFServingClient를 call합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'serving.kubeflow.org/v1alpha2',\n",
       " 'kind': 'InferenceService',\n",
       " 'metadata': {'creationTimestamp': '2020-09-26T21:15:50Z',\n",
       "  'generation': 1,\n",
       "  'name': 'mnist-server',\n",
       "  'namespace': 'koock',\n",
       "  'resourceVersion': '3024488',\n",
       "  'selfLink': '/apis/serving.kubeflow.org/v1alpha2/namespaces/koock/inferenceservices/mnist-server',\n",
       "  'uid': '705d807e-3a50-4e4d-8f9e-1354b322f03d'},\n",
       " 'spec': {'default': {'predictor': {'tensorflow': {'resources': {'limits': {'cpu': '100m',\n",
       "       'memory': '1Gi'},\n",
       "      'requests': {'cpu': '100m', 'memory': '1Gi'}},\n",
       "     'runtimeVersion': '1.14.0',\n",
       "     'storageUri': 'pvc://workspace-lecture-tf/saved_model'}}}},\n",
       " 'status': {}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KFServing = KFServingClient()\n",
    "KFServing.create(isvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 READY      DEFAULT_TRAFFIC CANARY_TRAFFIC  URL                                               \n",
      "mnist-server         False                                                                                        \n",
      "mnist-server         False                                                                                        \n",
      "mnist-server         True       100                             http://mnist-server.koock.example.com/v1/models...\n"
     ]
    }
   ],
   "source": [
    "KFServing.get('mnist-server', namespace=namespace, watch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['flatten_input'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 28, 28)\n",
      "      name: serving_default_flatten_input:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir ./saved_model/001 --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MODEL_NAME=mnist-server\n",
    "INPUT_PATH=@./mnist-input-9.json\n",
    "CLUSTER_IP=$(kubectl -n istio-system get service kfserving-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
    "SERVICE_HOSTNAME=$(kubectl get inferenceservice $MODEL_NAME -n koock -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n",
    "curl -H \"Host: ${SERVICE_HOSTNAME}\" -v http://$CLUSTER_IP/v1/models/$MODEL_NAME:predict -d $INPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFServing.delete('mnist-server', namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
